# FundPulse: Non-Profit Donation Analysis

**Analyzing donor behavior to identify repeat donors, donation frequency trends, and periods of high fundraising effectiveness.**

---

## ğŸ“‹ Quick Overview

FundPulse is a Data Science project focused on understanding donation patterns in non-profit organizations. By analyzing donation records, we identify:

- **Repeat Donors**: Who gives multiple times?
- **Frequency Trends**: How often do donors give?
- **High-Effectiveness Periods**: When is fundraising most successful?

This project demonstrates professional Data Science practices through clean organization, clear documentation, and reproducible analysis.

---

## ğŸ“ Project Structure

```
FundPulse/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ STRUCTURE.md                 # Detailed folder organization guide
â”œâ”€â”€ PROBLEM_STATEMENT.md         # Complete problem definition
â”‚
â”œâ”€â”€ data/                        # All data-related files
â”‚   â”œâ”€â”€ raw/                     # â† Original, untouched source data
â”‚   â”œâ”€â”€ processed/               # â† Cleaned, transformed data
â”‚   â””â”€â”€ README.md                # Data folder documentation
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks (primary analysis)
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_data_cleaning.ipynb
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ src/                         # Reusable Python modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ data_processor.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ outputs/                     # Generated results (plots, reports)
â”‚   â”œâ”€â”€ figures/                 # Charts and visualizations
â”‚   â”œâ”€â”€ reports/                 # Analysis summaries
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ configs/                     # Configuration and constants
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ docs/                        # Project documentation
    â”œâ”€â”€ DATA_DICTIONARY.md       # Data field definitions
    â”œâ”€â”€ METHODOLOGY.md           # Analysis approach
    â”œâ”€â”€ ASSUMPTIONS.md           # Explicit assumptions
    â””â”€â”€ README.md
```

---

## ğŸ”„ Data Lifecycle: Raw â†’ Processed â†’ Outputs

This project demonstrates **the three stages of data handling**â€”the foundation of reproducible, trustworthy Data Science:

### Stage 1: Raw Data (`data/raw/`)

**Purpose**: Store original, unmodified source data exactly as received.

âœ… **Golden Rule**: This folder is **READ-ONLY**. Never edit files here.

```
data/
â””â”€â”€ raw/
    â”œâ”€â”€ donations_raw_2024.csv          â† Original data file
    â””â”€â”€ README.md                        â† Document sources and dates
```

**Why?**
- Preserves evidence of original data
- Enables reproducibility (always start from same source)
- Protects against accidental corruption
- Creates audit trail of what data we started with

**Example:**
```python
# Loading raw data (read-only)
import pandas as pd
df_raw = pd.read_csv('data/raw/donations_raw_2024.csv')
# NEVER modify df_raw directlyâ€”create a copy if you need to clean
```

---

### Stage 2: Processed Data (`data/processed/`)

**Purpose**: Store cleaned, transformed datasets created from raw data.

```
data/
â””â”€â”€ processed/
    â”œâ”€â”€ donations_cleaned.csv            â† Processed from raw
    â”œâ”€â”€ donors_deduplicated.csv          â† Processed from raw
    â””â”€â”€ README.md                        â† Document transformations
```

**Why?**
- Separates raw from cleaned (clarity)
- Documents transformation steps
- Enables reproducibility (regenerate from raw anytime)
- Protects raw data from accidental modification

**Transformation Flow:**
```
data/raw/donations_raw_2024.csv
         â†“ (cleaning script)
data/processed/donations_cleaned.csv
         â†“ (analysis notebooks)
outputs/figures/  &  outputs/reports/
```

**Regenerating Processed Data:**
```python
# In a cleaning script/notebook:
df_raw = pd.read_csv('data/raw/donations_raw_2024.csv')

# Clean and transform
df_clean = df_raw.drop_duplicates()
df_clean['date'] = pd.to_datetime(df_clean['date'])

# Save to processed folder (never overwrite raw!)
df_clean.to_csv('data/processed/donations_cleaned.csv', index=False)

print("âœ“ Processed data saved. Raw data remains untouched.")
```

---

### Stage 3: Outputs (`outputs/`)

**Purpose**: Store final or intermediate results (plots, reports, models).

```
outputs/
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ donation_distribution.png
â”‚   â”œâ”€â”€ trend_by_month.pdf
â”‚   â””â”€â”€ seasonal_pattern.png
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ repeat_donor_analysis.md
â”‚   â”œâ”€â”€ fundraising_effectiveness.xlsx
â”‚   â””â”€â”€ summary_findings.pdf
â””â”€â”€ README.md
```

**Why This Is Separate from Data Folders:**
- Outputs are **generated results**, not input data
- Should never be committed to raw or processed
- Safe to delete and regenerate
- Easy to organize by type (figures, reports, models)

**Example Output Generation:**
```python
# In analysis notebook:
import matplotlib.pyplot as plt

# Generate visualization
plt.figure(figsize=(10, 6))
plt.hist(df['donation_amount'], bins=30)
plt.title('Donation Distribution')

# Save to outputs (NOT to data folder!)
plt.savefig('outputs/figures/donation_distribution.png', dpi=300)
plt.close()

print("âœ“ Figure saved. Data remains clean and separate.")
```

---

## âš ï¸ Critical Principles: Preventing Data Contamination

### âœ— What NOT to Do

**âŒ Problem 1: Modifying Raw Data**
```python
# WRONG! Modifies original file
df = pd.read_csv('data/raw/donations.csv')
df = df.drop_duplicates()  # Lost original duplicates!
df.to_csv('data/raw/donations.csv')  # Overwrites source!
```

**âœ… Solution:**
```python
# RIGHT! Preserves raw, creates processed
df_raw = pd.read_csv('data/raw/donations.csv')
df_clean = df_raw.drop_duplicates()
df_clean.to_csv('data/processed/donations_cleaned.csv')
```

---

**âŒ Problem 2: Mixing Outputs with Data**
```python
# WRONG! Output stored with input data
plt.savefig('data/raw/donation_distribution.png')  # âŒ Confuses files!
```

**âœ… Solution:**
```python
# RIGHT! Outputs separated
plt.savefig('outputs/figures/donation_distribution.png')  # âœ“ Clear
```

---

**âŒ Problem 3: Unclear Data Flow**
```
No documentation of:
- Where did processed data come from?
- What transformations were applied?
- Can we regenerate it?
```

**âœ… Solution:**
```
Each folder has README.md explaining:
- Source of raw data with dates
- Transformations applied to get processed
- How to regenerate if needed
```

---

## ğŸš€ Getting Started

### 1. Add Your Raw Data

Place your raw donation data file in `data/raw/`:

```bash
cp your_donations_data.csv data/raw/donations_raw_2024.csv
```

Then document it in `data/raw/README.md`:

```markdown
### Donations Data
- **File**: donations_raw_2024.csv
- **Source**: [Where it came from]
- **Date Received**: 2024-01-15
- **Records**: [Number of rows]
```

### 2. Create a Cleaning Script

Create a notebook or script that:
- Reads from `data/raw/`
- Cleans and transforms
- Saves to `data/processed/`

**Example: `notebooks/02_data_cleaning.ipynb`**

### 3. Run Analysis Notebooks

Create analysis notebooks in `notebooks/`:
- Read from `data/processed/`
- Perform analysis
- Save outputs to `outputs/figures/` and `outputs/reports/`

### 4. Verify the Flow

```
âœ“ data/raw/donations_raw_2024.csv     (unchanged)
  â†“
âœ“ data/processed/donations_cleaned.csv (regenerable from raw)
  â†“
âœ“ outputs/figures/*.png               (regenerable from processed)
âœ“ outputs/reports/*.md                (regenerable from processed)
```

---

## ğŸ“– Documentation Files

| File | Purpose |
|------|---------|
| **[STRUCTURE.md](./STRUCTURE.md)** | Detailed folder roles and naming conventions |
| **[PROBLEM_STATEMENT.md](./PROBLEM_STATEMENT.md)** | Project goals, success criteria, definitions |
| **[docs/DATA_DICTIONARY.md](./docs/DATA_DICTIONARY.md)** | Description of all data fields |
| **[docs/METHODOLOGY.md](./docs/METHODOLOGY.md)** | Analysis approach and techniques |
| **[docs/ASSUMPTIONS.md](./docs/ASSUMPTIONS.md)** | All explicit assumptions in the analysis |

---

## âœ… Checklist: Before Final Submission

- [ ] Raw data is safe in `data/raw/` (never modified)
- [ ] Processed data saved in `data/processed/` (can regenerate from raw)
- [ ] Outputs saved in `outputs/` (not mixed with data folders)
- [ ] Each folder has a README.md explaining its purpose
- [ ] Data flow is one-directional: raw â†’ processed â†’ outputs
- [ ] All scripts/notebooks read from correct source folder
- [ ] All scripts/notebooks write to correct destination folder
- [ ] Relative paths work from project root: `data/raw/...`
- [ ] Notebooks run top-to-bottom without errors
- [ ] Documentation is complete and current

---

## ğŸ”‘ Key Takeaway

**Raw data is evidence. Treat it like evidence.**

- Raw data should never be modified
- Processing steps should be documented and reproducible
- Outputs should be clearly separated from inputs
- This structure ensures trust, reproducibility, and professionalism

---

## ğŸ“š Learn More

- See [STRUCTURE.md](./STRUCTURE.md) for complete organization details
- See [PROBLEM_STATEMENT.md](./PROBLEM_STATEMENT.md) for project objectives
- See `docs/` folder for data dictionary, methodology, and assumptions

---

**Project**: S86-0226-DD Applied Data Science Foundation
**Topic**: Non-Profit Donation Analysis
**Status**: In Progress
**Last Updated**: February 27, 2026